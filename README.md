# Scene Recognition and Automatic Labeling

### Multimodal Vision Pipelines for Images & Videos

This project implements a **modular multimodal computer vision framework** for **scene recognition and automatic labeling** from **images and videos**.
It integrates **object detection, keypoint extraction, optical flow, action inference, and visionâ€“language captioning** into a single interpretable pipeline.

The system produces:

* Annotated images (bounding boxes, keypoints, motion cues)
* Per-object and global natural-language captions
* Frame-wise annotated video outputs (optional)

---

## ğŸ” Overview

**Pipeline stages**

1. **Object Detection** â€“ Detects objects with confidence filtering
2. **Keypoint Extraction** â€“ Classical feature extraction (SIFT / ORB)
3. **Optical Flow (Optional)** â€“ Motion estimation from frame pairs
4. **Action Inference** â€“ Rule-based reasoning using IoU + motion cues
5. **Caption Generation** â€“ Global & object-level captions using BLIP
6. **Visualization** â€“ Unified annotated output

This hybrid design combines **classical vision** and **deep learning**, keeping the system **interpretable and extensible**.

---

## ğŸ“ Project Structure

```text
Scene-Recognition-And-Automatic-Labeling/
â”‚
â”œâ”€â”€ data/                     # Input images / video frames
â”‚   â”œâ”€â”€ image4.jpg
â”‚   â”œâ”€â”€ frame_000.jpg
â”‚   â””â”€â”€ frame_001.jpg
â”‚
â”œâ”€â”€ output/                   # Generated results
â”‚   â””â”€â”€ annotated.jpg
â”‚
â”œâ”€â”€ src/                       # Core vision modules
â”‚   â”œâ”€â”€ detect.py              # Object detection (Mask R-CNN wrapper)
â”‚   â”œâ”€â”€ features.py            # Keypoint extraction (SIFT / ORB)
â”‚   â”œâ”€â”€ flow.py                # Dense optical flow (Farneback)
â”‚   â”œâ”€â”€ actions.py             # Action inference (IoU + motion rules)
â”‚   â”œâ”€â”€ caption.py             # BLIP-based caption generation
â”‚   â”œâ”€â”€ viz.py                 # Annotation & visualization utilities
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ run_test.py                # Main driver script
â”œâ”€â”€ WRITE-UP.pdf               # Project report / methodology
â””â”€â”€ README.md
```

---

## â–¶ï¸ How the Pipeline Works

```text
Input Image / Frames
        â†“
Object Detection (Mask R-CNN)
        â†“
Keypoint Extraction (SIFT / ORB)
        â†“
Optical Flow (if frame pair available)
        â†“
Action Inference (heuristic rules)
        â†“
Caption Generation (BLIP)
        â†“
Annotated Output (Image / Video)
```

---

## ğŸš€ Running the Project

### 1ï¸âƒ£ Create a Virtual Environment (Recommended)

```bash
python -m venv venv
```

Activate it:

**Windows**

```bash
venv\Scripts\activate
```

**Linux / macOS**

```bash
source venv/bin/activate
```

---

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```
---

### 3ï¸âƒ£ Run the Demo Script

```bash
python run_test.py
```

What it does:

* Loads input image from `data/`
* Runs all vision modules from `src/`
* Generates captions and annotations
* Saves output to `output/annotated.jpg`

---

## ğŸ“Š Output Examples

### ğŸ–¼ï¸ Image Output
![Image](https://github.com/user-attachments/assets/64337aaf-dff5-42d8-af89-25ea80249fdf)

---

### ğŸ¥ Video Output

For videos, the pipeline processes **each frame independently** and saves annotated frames.
https://github.com/user-attachments/assets/30d0d7b2-0b1e-4bcb-9829-e30a56c7dbb1

---

## ğŸ““ Jupyter Notebook Support (VS Code)

### Install Jupyter Kernel Inside venv

```bash
pip install ipykernel
```

Register the virtual environment as a kernel:

```bash
python -m ipykernel install --user --name scene-vision-venv --display-name "SceneVision (venv)"
```

### Use in VS Code

1. Open a `.ipynb` file
2. Click **Select Kernel**
3. Choose **SceneVision (venv)**

This ensures notebooks use the **same environment as the project**.

---

## ğŸ§  Key Design Choices

* **Hybrid approach**: Classical vision + deep learning
* **Interpretability**: Explicit action rules instead of black-box HOI
* **Modularity**: Each vision task isolated in `src/`
* **Extensibility**:

  * Replace heuristics with learned HOI models
  * Swap BLIP with other visionâ€“language models
  * Extend to temporal video captioning

---

## âš ï¸ Known Limitations

* Small objects may be missed at higher detection thresholds
* Optical flow can be unstable in low-texture regions
* Action inference is heuristic-based (rule-driven)
* Video processing generates large numbers of annotated frames

---

## ğŸ“„ Reference

For full methodology, experiments, and analysis, see:

**`WRITE-UP.pdf`** â€“ *Scene Recognition and Automatic Labeling from Images Using Multimodal Vision Pipelines*

---

## ğŸ‘¤ Author

**Bhaskar R**
Computer Vision & Multimodal AI
*(Academic project â€“ EE604 / Image Processing)*
