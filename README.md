# Scene Recognition and Automatic Labeling

### Multimodal Vision Pipelines for Images & Videos

This project implements a **modular multimodal computer vision framework** for **scene recognition and automatic labeling** from **images and videos**.
It integrates **object detection, keypoint extraction, optical flow, action inference, and vision‚Äìlanguage captioning** into a single interpretable pipeline.

The system produces:

* Annotated images (bounding boxes, keypoints, motion cues)
* Per-object and global natural-language captions
* Frame-wise annotated video outputs (optional)

---

## üîç Overview

**Pipeline stages**

1. **Object Detection** ‚Äì Detects objects with confidence filtering
2. **Keypoint Extraction** ‚Äì Classical feature extraction (SIFT / ORB)
3. **Optical Flow (Optional)** ‚Äì Motion estimation from frame pairs
4. **Action Inference** ‚Äì Rule-based reasoning using IoU + motion cues
5. **Caption Generation** ‚Äì Global & object-level captions using BLIP
6. **Visualization** ‚Äì Unified annotated output

This hybrid design combines **classical vision** and **deep learning**, keeping the system **interpretable and extensible**.

---

## üìÅ Project Structure

```text
Scene-Recognition-And-Automatic-Labeling/
‚îÇ
‚îú‚îÄ‚îÄ data/                     # Input images / video frames
‚îÇ   ‚îú‚îÄ‚îÄ image4.jpg
‚îÇ   ‚îú‚îÄ‚îÄ frame_000.jpg
‚îÇ   ‚îî‚îÄ‚îÄ frame_001.jpg
‚îÇ
‚îú‚îÄ‚îÄ output/                   # Generated results
‚îÇ   ‚îî‚îÄ‚îÄ annotated.jpg
‚îÇ
‚îú‚îÄ‚îÄ src/                       # Core vision modules
‚îÇ   ‚îú‚îÄ‚îÄ detect.py              # Object detection (Mask R-CNN wrapper)
‚îÇ   ‚îú‚îÄ‚îÄ features.py            # Keypoint extraction (SIFT / ORB)
‚îÇ   ‚îú‚îÄ‚îÄ flow.py                # Dense optical flow (Farneback)
‚îÇ   ‚îú‚îÄ‚îÄ actions.py             # Action inference (IoU + motion rules)
‚îÇ   ‚îú‚îÄ‚îÄ caption.py             # BLIP-based caption generation
‚îÇ   ‚îú‚îÄ‚îÄ viz.py                 # Annotation & visualization utilities
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ
‚îú‚îÄ‚îÄ run_test.py                # Main driver script
‚îú‚îÄ‚îÄ WRITE-UP.pdf               # Project report / methodology
‚îî‚îÄ‚îÄ README.md
```

---

## ‚ñ∂Ô∏è How the Pipeline Works

```text
Input Image / Frames
        ‚Üì
Object Detection (Mask R-CNN)
        ‚Üì
Keypoint Extraction (SIFT / ORB)
        ‚Üì
Optical Flow (if frame pair available)
        ‚Üì
Action Inference (heuristic rules)
        ‚Üì
Caption Generation (BLIP)
        ‚Üì
Annotated Output (Image / Video)
```

---

## üöÄ Running the Project

### 1Ô∏è‚É£ Create a Virtual Environment (Recommended)

```bash
python -m venv venv
```

Activate it:

**Windows**

```bash
venv\Scripts\activate
```

**Linux / macOS**

```bash
source venv/bin/activate
```

---

### 2Ô∏è‚É£ Install Dependencies

```bash
pip install -r requirements.txt
```

(If `requirements.txt` is not created yet, install manually:
`torch`, `torchvision`, `opencv-python`, `Pillow`, `matplotlib`, `transformers`)

---

### 3Ô∏è‚É£ Run the Demo Script

```bash
python run_test.py
```

What it does:

* Loads input image from `data/`
* Runs all vision modules from `src/`
* Generates captions and annotations
* Saves output to `output/annotated.jpg`

---

## üìä Output Examples

### üñºÔ∏è Image Output

> *(Paste your annotated image here)*

```markdown
![Annotated Output](output/annotated.jpg)
```

---

### üé• Video Output

For videos, the pipeline processes **each frame independently** and saves annotated frames.

> *(Paste your video demo or GIF here)*

```markdown
![Video Demo](assets/demo_video.gif)
```

*(Full videos are not embedded due to size constraints.)*

---

## üìì Jupyter Notebook Support (VS Code)

### Install Jupyter Kernel Inside venv

```bash
pip install ipykernel
```

Register the virtual environment as a kernel:

```bash
python -m ipykernel install --user --name scene-vision-venv --display-name "SceneVision (venv)"
```

### Use in VS Code

1. Open a `.ipynb` file
2. Click **Select Kernel**
3. Choose **SceneVision (venv)**

This ensures notebooks use the **same environment as the project**.

---

## üß† Key Design Choices

* **Hybrid approach**: Classical vision + deep learning
* **Interpretability**: Explicit action rules instead of black-box HOI
* **Modularity**: Each vision task isolated in `src/`
* **Extensibility**:

  * Replace heuristics with learned HOI models
  * Swap BLIP with other vision‚Äìlanguage models
  * Extend to temporal video captioning

---

## ‚ö†Ô∏è Known Limitations

* Small objects may be missed at higher detection thresholds
* Optical flow can be unstable in low-texture regions
* Action inference is heuristic-based (rule-driven)
* Video processing generates large numbers of annotated frames

---

## üìÑ Reference

For full methodology, experiments, and analysis, see:

**`WRITE-UP.pdf`** ‚Äì *Scene Recognition and Automatic Labeling from Images Using Multimodal Vision Pipelines*

---

## üë§ Author

**Bhaskar R**
Computer Vision & Multimodal AI
*(Academic project ‚Äì EE604 / Image Processing)*
